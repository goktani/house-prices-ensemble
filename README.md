# üè† House Prices ‚Äî Advanced Regression Techniques

> Kaggle competition solution achieving **~0.111 RMSE** (Top 10%)  
> End-to-end ML pipeline with stacking ensemble: Ridge ¬∑ Lasso ¬∑ XGBoost ¬∑ LightGBM ¬∑ CatBoost

---

## üìå Competition

**[House Prices: Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)**

- **Task:** Predict residential home sale prices in Ames, Iowa
- **Dataset:** 79 explanatory variables describing (almost) every aspect of residential homes
- **Metric:** RMSLE ‚Äî Root Mean Squared Log Error

---

## üìÅ Repository Structure

```
house-prices-ensemble/
‚îÇ
‚îú‚îÄ‚îÄ house_prices_kaggle.ipynb   # Main pipeline notebook (Kaggle-ready)
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îî‚îÄ‚îÄ README.md
```

---

## üîß Pipeline Overview

```
Raw Data (1460 rows, 79 features)
        ‚îÇ
        ‚îú‚îÄ‚îÄ 1. Outlier Removal       (2 rows only ‚Äî GrLivArea anomalies)
        ‚îú‚îÄ‚îÄ 2. Missing Value Strategy (semantic NA ‚Üí "None", LotFrontage ‚Üí neighborhood median)
        ‚îú‚îÄ‚îÄ 3. Ordinal Encoding       (quality/condition columns ‚Üí ranked integers)
        ‚îú‚îÄ‚îÄ 4. Feature Engineering    (30+ new features)
        ‚îÇ     ‚îú‚îÄ‚îÄ Area aggregations   (TotalSF, TotalBathrooms, TotalPorchSF)
        ‚îÇ     ‚îú‚îÄ‚îÄ Time features       (HouseAge, IsRemodeled, YearsSinceRemod)
        ‚îÇ     ‚îú‚îÄ‚îÄ Quality √ó Area      (QualArea, BsmtScore, KitchenScore)
        ‚îÇ     ‚îú‚îÄ‚îÄ Polynomial terms    (OverallQual¬≤, OverallQual¬≥)
        ‚îÇ     ‚îî‚îÄ‚îÄ Boolean flags       (HasPool, HasGarage, HasFireplace...)
        ‚îú‚îÄ‚îÄ 5. One-Hot Encoding       (nominal categoricals)
        ‚îú‚îÄ‚îÄ 6. Box-Cox Transform      (skewed continuous features, |skew| > 0.5)
        ‚îî‚îÄ‚îÄ 7. Variance Filter        (remove near-zero variance features)
                ‚îÇ
                ‚ñº
        Level-1 Models (10-Fold OOF)
        ‚îú‚îÄ‚îÄ Ridge          OOF RMSE: 0.11005
        ‚îú‚îÄ‚îÄ Lasso          OOF RMSE: 0.11018
        ‚îú‚îÄ‚îÄ ElasticNet     OOF RMSE: 0.11022
        ‚îú‚îÄ‚îÄ CatBoost       OOF RMSE: 0.11391
        ‚îú‚îÄ‚îÄ XGBoost        OOF RMSE: 0.11569
        ‚îî‚îÄ‚îÄ LightGBM       OOF RMSE: 0.11793
                ‚îÇ
                ‚ñº
        Level-2 Ensemble
        ‚îú‚îÄ‚îÄ Stacking  (RidgeCV meta-model)  OOF RMSE: 0.10843
        ‚îú‚îÄ‚îÄ Blending  (scipy-optimized weights) OOF RMSE: 0.10830
        ‚îî‚îÄ‚îÄ Final     (10% Stack + 90% Blend)   OOF RMSE: 0.10830
```

---

## üöÄ Quick Start

### Local

```bash
# 1. Clone the repo
git clone https://github.com/goktani/house-prices-ensemble.git
cd house-prices-ensemble

# 2. Install dependencies
pip install -r requirements.txt

# 3. Download data from Kaggle
kaggle competitions download -c house-prices-advanced-regression-techniques
unzip house-prices-advanced-regression-techniques.zip

# 4. Run the notebook
jupyter notebook house_prices_kaggle.ipynb
```

### Kaggle Notebook

1. Go to [Kaggle Notebooks](https://www.kaggle.com/code)
2. Click **New Notebook ‚Üí File ‚Üí Import Notebook**
3. Upload `house_prices_kaggle.ipynb`
4. Add the competition dataset from the **Data** panel
5. Click **Run All**

---

## üìä Results

| Model | OOF RMSE |
|-------|----------|
| Ridge | 0.11005 |
| Lasso | 0.11018 |
| ElasticNet | 0.11022 |
| CatBoost | 0.11391 |
| XGBoost | 0.11569 |
| LightGBM | 0.11793 |
| **Stacking Ensemble** | **0.10843** |
| **Blending Ensemble** | **0.10830** |
| **Final (Blend 90% + Stack 10%)** | **0.10830** |

---

## üí° Key Design Decisions

**Outlier Removal ‚Äî Why only 2 rows?**  
The competition host explicitly recommends removing only the 2 houses with `GrLivArea > 4000` that sold abnormally cheap. Removing more rows causes the model to never see high-value homes (`$300k+`), capping predictions at ~$280k.

**Ordinal vs One-Hot Encoding**  
Quality/condition columns (`ExterQual`, `KitchenQual`, etc.) are ordinal ‚Äî `Ex > Gd > TA > Fa > Po`. Encoding them as ranked integers preserves this hierarchy, especially beneficial for linear models.

**Why Blending Dominates (Œ±=0.90)?**  
Ridge/Lasso have very strong OOF scores (0.110) and the scipy-optimized blending assigns them ~65% combined weight. The stacking meta-model adds marginal improvement, but simple weighted blending is more robust here.

**LotFrontage Strategy**  
Filled with neighborhood median rather than global median ‚Äî homes on the same street tend to have similar frontage widths.

---

## üõ†Ô∏è Tech Stack

| Library | Version | Purpose |
|---------|---------|---------|
| scikit-learn | 1.4.2 | Linear models, CV, preprocessing |
| XGBoost | 2.0.3 | Gradient boosting |
| LightGBM | 4.3.0 | Gradient boosting (fast) |
| CatBoost | 1.2.5 | Gradient boosting (ordered) |
| Optuna | 3.6.1 | Hyperparameter optimization |
| SciPy | 1.13.0 | Box-Cox transform, blend optimization |
| Pandas | 2.2.2 | Data manipulation |

---

## üìú License

MIT License ‚Äî feel free to use, modify, and distribute.
